{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f3ef6c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import pandas as pd\n",
    "from googlesearch import search\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dcd7fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_timeout = 10\n",
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:49.0) Gecko/20100101 Firefox/49.0',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Cache-Control': 'private, max-age=0',\n",
    "}\n",
    "html_pattern = \"<(?:\\\"[^\\\"]*\\\"['\\\"]*|'[^']*'['\\\"]*|[^'\\\">])+>\"\n",
    "\n",
    "#search variables\n",
    "from_year = 2013\n",
    "to_year = 2022\n",
    "total_news_per_year = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ceba417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_google_news_urls(q, page, from_year, to_year):\\n    urls = []\\n    queries_tag = \"Gx5Zad fP1Qef xpd EtOod pkphOe\"\\n    title_tag = \"BNeawe vvjwJb AP7Wnd\"\\n    search_url = f\\'https://www.google.com/search?q={q}&lr=lang_en&cd_min=1/1/2015&cd_max=2013&tbs=sbd:1,lr:lang_en,cdr:1,cd_min:1/1/2015,cd_max:2015&tbm=nws&start={page * 10}\\'\\n    print(search_url)\\n    search_result = requests.get(search_url)\\n    soup = bs(search_result.content, \"html.parser\")\\n    body = soup.find(id=\"main\")\\n    queries = body.find_all(\"div\", queries_tag)\\n    for i in queries:\\n        output = {}\\n        header = i.find(\\'a\\')\\n        title = header.find(\\'div\\', \\'BNeawe vvjwJb AP7Wnd\\').text\\n        url = header[\\'href\\']\\n        url = url.replace(\\'/url?q=\\', \\'\\')\\n        url = url[0: s.index(\"&sa\")]\\n        output[\\'url\\'] = url\\n        output[\\'title\\'] = title\\n        urls.append(output)    \\n        \\n    return urls\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def get_google_news_urls(q, page, from_year, to_year):\n",
    "    urls = []\n",
    "    queries_tag = \"Gx5Zad fP1Qef xpd EtOod pkphOe\"\n",
    "    title_tag = \"BNeawe vvjwJb AP7Wnd\"\n",
    "    search_url = f'https://www.google.com/search?q={q}&lr=lang_en&cd_min=1/1/2015&cd_max=2013&tbs=sbd:1,lr:lang_en,cdr:1,cd_min:1/1/2015,cd_max:2015&tbm=nws&start={page * 10}'\n",
    "    print(search_url)\n",
    "    search_result = requests.get(search_url)\n",
    "    soup = bs(search_result.content, \"html.parser\")\n",
    "    body = soup.find(id=\"main\")\n",
    "    queries = body.find_all(\"div\", queries_tag)\n",
    "    for i in queries:\n",
    "        output = {}\n",
    "        header = i.find('a')\n",
    "        title = header.find('div', 'BNeawe vvjwJb AP7Wnd').text\n",
    "        url = header['href']\n",
    "        url = url.replace('/url?q=', '')\n",
    "        url = url[0: s.index(\"&sa\")]\n",
    "        output['url'] = url\n",
    "        output['title'] = title\n",
    "        urls.append(output)    \n",
    "        \n",
    "    return urls\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fde9b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_news_urls(q, total, from_year, to_year):\n",
    "    urls = []\n",
    "    for url in search(q, tld=\"co.in\", start=0, stop=total, num=20, pause=2, lang='en', tbs=f'sbd:1,lr:lang_en,cdr:1,cd_min:1/1/{from_year},cd_max:{to_year}', extra_params={'tbm': 'nws'}):\n",
    "        urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91f42a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_data(url):\n",
    "    output = {}\n",
    "    text = \"\"\n",
    "    title = \"\"\n",
    "    datetime = \"\"\n",
    "    try:\n",
    "        article = requests.get(url, headers=headers, timeout=request_timeout)\n",
    "        status_code = article.status_code\n",
    "        if status_code >= 400:\n",
    "            f = open('ERROR_LOG.txt', 'a')\n",
    "            f.write(f'[{time.asctime(time.localtime())}] Code {status_code}: {url}\\n')\n",
    "            f.close()\n",
    "            return None\n",
    "        soup = bs(article.content, \"html.parser\")\n",
    "        datetime = soup.find('meta', property='article:published_time')\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        if soup.find('title') is not None:\n",
    "            title = soup.find('title').text\n",
    "        if datetime is not None:\n",
    "            datetime = datetime['content']\n",
    "        if paragraphs is not None:\n",
    "            for p in paragraphs:\n",
    "                text += re.sub(html_pattern, '', p.text).strip() + ' '  \n",
    "        output['title'] = title\n",
    "        output['text'] = text\n",
    "        output['datetime'] = datetime\n",
    "        output['url'] = url\n",
    "        return output\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        msg = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ef8e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(q, urls, year):\n",
    "    total_count = 0\n",
    "    for url in urls:\n",
    "        try:\n",
    "            data = get_news_data(url)\n",
    "            if data is None:\n",
    "                continue\n",
    "            if q not in os.listdir():\n",
    "                cwd = os.getcwd()\n",
    "                os.mkdir(f'{cwd}/{q}')\n",
    "            total_count += 1\n",
    "            file_error_symbols = []\n",
    "            filename = f'{year}-{total_count}'\n",
    "            f = open(f'./{q}/{filename}.json', \"w\")\n",
    "            f.write(json.dumps(data, indent = 4))\n",
    "            f.close()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(e)\n",
    "    print(f'Saved {total_count} news data in year {year}')\n",
    "    #print(os.system(\"npx prettier -w ./dataset/*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b254fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_news_urls(q, total, from_year, to_year):\n",
    "    urls = {}\n",
    "    for year in range(from_year, to_year + 1):\n",
    "        urls[str(year)] = get_google_news_urls(q, total, year, year)\n",
    "        print(f'Year {year} News urls collection - Done')\n",
    "        #delay request to prevent 429 Too many Request\n",
    "        if total > 200:\n",
    "            time.sleep(60)\n",
    "        else:\n",
    "            time.sleep(30)\n",
    "            \n",
    "    f = open(f'{q}.json', 'w')\n",
    "    f.write(json.dumps(urls, indent = 4))\n",
    "    f.close()\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c56ef85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_from_file(filename):\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fdadc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_from_file(filename):\n",
    "    keywords = []\n",
    "    f = open(filename, \"r\")\n",
    "    for keyword in f:\n",
    "        keywords.append(keyword)\n",
    "    f.close()\n",
    "    return list(map(lambda keyword: keyword.strip(), keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "349e49da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_save_dataset(q, urls):\n",
    "    start = time.time()\n",
    "    Parallel(n_jobs=-1)(delayed(save_dataset)(q, urls[year], year) for year in urls)\n",
    "    end = time.time()\n",
    "    print('{:.2f} seconds used'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deaf2513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run all functions to save news data\n",
    "def scrap_news_data(total, from_year, to_year, keywords_filename=\"keywords.txt\"):\n",
    "    #count used time\n",
    "    start = time.time()\n",
    "    #step 1 - load keywords file\n",
    "    keywords = get_keywords_from_file(keywords_filename)\n",
    "    for query in keywords:\n",
    "        print(f'Scrapping {query} ...')\n",
    "        #step 2 - save urls to json file\n",
    "        save_news_urls(query, total, from_year, to_year)\n",
    "        #step 3 - get urls object from previous saved json file\n",
    "        urls = get_urls_from_file(f'{query}.json')\n",
    "        #step 4 - web scrapping news data from specific year of urls & save into directory seperately\n",
    "        parallel_save_dataset(query, urls)\n",
    "        \n",
    "    end = time.time()\n",
    "    print(f'[DONE] {end - start} seconds used')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "84a37454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_csv(q, json_obj, index):\n",
    "    title = re.sub(r'[,.\\'\"|]', '', json_obj['title'])\n",
    "    datetime = json_obj['datetime']\n",
    "    url = json_obj['url']\n",
    "    text = re.sub(r'[,.\\'\"|]', '', json_obj['text'])\n",
    "    if text != '':\n",
    "        text = ' '.join(text.encode('utf-8').decode().split())\n",
    "    if title != '':\n",
    "        title = ' '.join(title.split()).strip()\n",
    "    f = open(f'{q}.csv', 'a', encoding='utf-8')\n",
    "    f.write(f'{index},\\\"{title}\\\",{datetime},{url},\\\"{text}\\\"\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "0761ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_json_to_csv():\n",
    "    cols = \"title,datetime,url,text\\n\"\n",
    "    keywords = get_keywords_from_file('keywords.txt')\n",
    "    for keyword in keywords:\n",
    "        f = open(f'{keyword}.csv', 'w')\n",
    "        f.write(cols)\n",
    "        f.close()\n",
    "        for index, filename in enumerate(os.listdir(f'./{keyword}')):\n",
    "            json_file = open(f'./{keyword}/{filename}', 'r')\n",
    "            json_obj = json.load(json_file)\n",
    "            json_file.close()\n",
    "            json_to_csv(keyword, json_obj, index)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "eb9ddf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv():\n",
    "    keywords = get_keywords_from_file('keywords.txt')\n",
    "    dataframes = []\n",
    "    for keyword in keywords:\n",
    "        df = pd.read_csv(f'{keyword}.csv', on_bad_lines='skip')\n",
    "        dataframes.append(df)\n",
    "    dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "    dataframe.to_csv('dataset.csv')\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "e0568eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrap_news_data(total_news_per_year, from_year, to_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "252920be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "all_json_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "beb90401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "combine_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "5ddd8126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>datetime</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‘KCON All Things Hallyu’ returns to LA</td>\n",
       "      <td>2013-06-27T15:15:35+09:00</td>\n",
       "      <td>https://www.koreaherald.com/view.php?ud=201306...</td>\n",
       "      <td>Most Popular South Korean celebrities grilled ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Choi Ji-woo to play modern-day Mary Poppins</td>\n",
       "      <td>2013-09-16T15:49:54+09:00</td>\n",
       "      <td>https://www.koreaherald.com/view.php?ud=201309...</td>\n",
       "      <td>Most Popular South Korean celebrities grilled ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4 Idola Hallyu Ini Dibilang Paling Mirip Bonek...</td>\n",
       "      <td>2013-12-07T22:00:00+07:00</td>\n",
       "      <td>https://www.liputan6.com/showbiz/read/767787/4...</td>\n",
       "      <td>liputan6 Diperbarui 07 Des 2013 22:00 WIBDiter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JYP Pun Siap Kembali Ramaikan Dunia Hallyu - S...</td>\n",
       "      <td>2013-08-29T13:20:00+07:00</td>\n",
       "      <td>https://www.liputan6.com/showbiz/read/678005/j...</td>\n",
       "      <td>liputan6 Diperbarui 29 Agu 2013 13:20 WIBDiter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Japan honors Jane Birkin for tsunami relief ef...</td>\n",
       "      <td>2013-11-15T15:00:55+09:00</td>\n",
       "      <td>https://www.koreaherald.com/view.php?ud=201311...</td>\n",
       "      <td>Most Popular South Korean celebrities grilled ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>[CELEB] From liquor to labels Jay Park knows n...</td>\n",
       "      <td>2022-03-09T14:55:44+0900</td>\n",
       "      <td>https://koreajoongangdaily.joins.com/2022/03/0...</td>\n",
       "      <td>Singer rapper and entrepreneur Jay Park [MORE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9102</th>\n",
       "      <td>1300: Sydney’s Korean-Australian hip-hop sensa...</td>\n",
       "      <td>2022-05-26T01:30:00+00:00</td>\n",
       "      <td>https://www.smh.com.au/culture/music/korean-au...</td>\n",
       "      <td>We’re sorry this feature is currently unavaila...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9103</th>\n",
       "      <td>What time will Hip Hop Homicides air on WE tv?...</td>\n",
       "      <td>2022-11-02T20:10:24+00:00</td>\n",
       "      <td>https://www.sportskeeda.com/pop-culture/what-t...</td>\n",
       "      <td>Curtis 50 Cent Jacksons highly anticipated eig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9104</th>\n",
       "      <td>BIBI: Meet The Enigmatic Princess of K-R&amp;B</td>\n",
       "      <td>2022-05-28T13:01:28+00:00</td>\n",
       "      <td>https://rollingstoneindia.com/bibi-meet-the-en...</td>\n",
       "      <td>She grabbed the world’s attention thanks to he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9105</th>\n",
       "      <td>2022 highlights of K-Pop: From rise of new fem...</td>\n",
       "      <td>2022-12-26T15:31:35+09:00</td>\n",
       "      <td>https://www.koreaherald.com/view.php?ud=202212...</td>\n",
       "      <td>Most Popular South Korean celebrities grilled ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9106 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0                ‘KCON All Things Hallyu’ returns to LA   \n",
       "1           Choi Ji-woo to play modern-day Mary Poppins   \n",
       "2     4 Idola Hallyu Ini Dibilang Paling Mirip Bonek...   \n",
       "3     JYP Pun Siap Kembali Ramaikan Dunia Hallyu - S...   \n",
       "4     Japan honors Jane Birkin for tsunami relief ef...   \n",
       "...                                                 ...   \n",
       "9101  [CELEB] From liquor to labels Jay Park knows n...   \n",
       "9102  1300: Sydney’s Korean-Australian hip-hop sensa...   \n",
       "9103  What time will Hip Hop Homicides air on WE tv?...   \n",
       "9104         BIBI: Meet The Enigmatic Princess of K-R&B   \n",
       "9105  2022 highlights of K-Pop: From rise of new fem...   \n",
       "\n",
       "                       datetime  \\\n",
       "0     2013-06-27T15:15:35+09:00   \n",
       "1     2013-09-16T15:49:54+09:00   \n",
       "2     2013-12-07T22:00:00+07:00   \n",
       "3     2013-08-29T13:20:00+07:00   \n",
       "4     2013-11-15T15:00:55+09:00   \n",
       "...                         ...   \n",
       "9101   2022-03-09T14:55:44+0900   \n",
       "9102  2022-05-26T01:30:00+00:00   \n",
       "9103  2022-11-02T20:10:24+00:00   \n",
       "9104  2022-05-28T13:01:28+00:00   \n",
       "9105  2022-12-26T15:31:35+09:00   \n",
       "\n",
       "                                                    url  \\\n",
       "0     https://www.koreaherald.com/view.php?ud=201306...   \n",
       "1     https://www.koreaherald.com/view.php?ud=201309...   \n",
       "2     https://www.liputan6.com/showbiz/read/767787/4...   \n",
       "3     https://www.liputan6.com/showbiz/read/678005/j...   \n",
       "4     https://www.koreaherald.com/view.php?ud=201311...   \n",
       "...                                                 ...   \n",
       "9101  https://koreajoongangdaily.joins.com/2022/03/0...   \n",
       "9102  https://www.smh.com.au/culture/music/korean-au...   \n",
       "9103  https://www.sportskeeda.com/pop-culture/what-t...   \n",
       "9104  https://rollingstoneindia.com/bibi-meet-the-en...   \n",
       "9105  https://www.koreaherald.com/view.php?ud=202212...   \n",
       "\n",
       "                                                   text  \n",
       "0     Most Popular South Korean celebrities grilled ...  \n",
       "1     Most Popular South Korean celebrities grilled ...  \n",
       "2     liputan6 Diperbarui 07 Des 2013 22:00 WIBDiter...  \n",
       "3     liputan6 Diperbarui 29 Agu 2013 13:20 WIBDiter...  \n",
       "4     Most Popular South Korean celebrities grilled ...  \n",
       "...                                                 ...  \n",
       "9101  Singer rapper and entrepreneur Jay Park [MORE ...  \n",
       "9102  We’re sorry this feature is currently unavaila...  \n",
       "9103  Curtis 50 Cent Jacksons highly anticipated eig...  \n",
       "9104  She grabbed the world’s attention thanks to he...  \n",
       "9105  Most Popular South Korean celebrities grilled ...  \n",
       "\n",
       "[9106 rows x 4 columns]"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb55d6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
